{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJelQc_9DDiR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from collections import Counter\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkCubDqOwsLO"
      },
      "outputs": [],
      "source": [
        "commit_sharings = \"./20231012_230826_commit_sharings.json\"\n",
        "pull_request_sharings = \"./20231012_233628_pr_sharings.json\"\n",
        "issue_sharings = \"./20231012_235128_issue_sharings.json\"\n",
        "file_sharings = \"./20231012_234250_file_sharings.json\"\n",
        "discussion_sharings = \"./20231012_235320_discussion_sharings.json\"\n",
        "\n",
        "path_list_synonym = \"./synonyms_for_normalization.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0H1Vn_9DSQq"
      },
      "outputs": [],
      "source": [
        "with open(commit_sharings, 'r') as f:\n",
        "    json1 = json.load(f)\n",
        "\n",
        "# Abra o arquivo json\n",
        "with open(pull_request_sharings, 'r') as f:\n",
        "    json2 = json.load(f)\n",
        "\n",
        "# Abra o arquivo json\n",
        "with open(issue_sharings, 'r') as f:\n",
        "    json3 = json.load(f)\n",
        "\n",
        "# Abra o arquivo json\n",
        "with open(file_sharings, 'r') as f:\n",
        "    json4 = json.load(f)\n",
        "\n",
        "# Abra o arquivo json\n",
        "with open(discussion_sharings, 'r') as f:\n",
        "    json5 = json.load(f)\n",
        "\n",
        "jsons = {\n",
        "    \"data1\": json1,\n",
        "    \"data2\": json2,\n",
        "    \"data3\": json3,\n",
        "    \"data4\": json4,\n",
        "    \"data5\": json5\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUf5s_I2gNzI"
      },
      "source": [
        "# Normalisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xRBwdwtPL5g"
      },
      "outputs": [],
      "source": [
        "with open(path_list_synonym, 'r') as file:\n",
        "    synonyms = {}\n",
        "\n",
        "    for line in file:\n",
        "        key, value = line.strip().split('=')\n",
        "        synonyms[key.strip()] = value.strip().split(',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQ2Jn8hvG4ub"
      },
      "outputs": [],
      "source": [
        "\n",
        "for DataTable in jsons:\n",
        "  for elements in jsons[DataTable][\"Sources\"]:\n",
        "    if elements[\"RepoLanguage\"] == None :\n",
        "      elements[\"RepoLanguage\"] = \"none\"\n",
        "    else:\n",
        "      for language, alias in synonyms.items():\n",
        "        if elements[\"RepoLanguage\"] in alias:\n",
        "          elements[\"RepoLanguage\"] = language\n",
        "\n",
        "for DataTable in jsons:\n",
        "  for elements in jsons[DataTable][\"Sources\"]:\n",
        "    for element in elements[\"ChatgptSharing\"]:\n",
        "      if \"Conversations\" in element and element[\"Conversations\"]:\n",
        "        for conversation in element[\"Conversations\"]:\n",
        "          if \"ListOfCode\" in conversation and conversation[\"ListOfCode\"]:\n",
        "            for snipetType in conversation[\"ListOfCode\"]:\n",
        "              if snipetType[\"Type\"] == None :\n",
        "                snipetType[\"Type\"] = \"none\"\n",
        "              else:\n",
        "                for language, alias in synonyms.items():\n",
        "                  if  snipetType[\"Type\"] in alias:\n",
        "                    snipetType[\"Type\"] = language\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyzWMR08Ayrq"
      },
      "outputs": [],
      "source": [
        "\n",
        "# file_path = './normalised/multifile_normalised.json'\n",
        "\n",
        "# with open(file_path, 'w') as file:\n",
        "#   json.dump(jsons, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpGcZcGYf9wg"
      },
      "source": [
        "# Data Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLwJtazqGgIJ"
      },
      "outputs": [],
      "source": [
        "repoLang = []\n",
        "for DataTable in jsons:\n",
        "  for elements in jsons[DataTable][\"Sources\"]:\n",
        "    repoLang.append(elements[\"RepoLanguage\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkrJphY-XhDI"
      },
      "outputs": [],
      "source": [
        "counting = Counter(repoLang)\n",
        "\n",
        "countingDict = dict(counting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeKQaHupXo9R"
      },
      "outputs": [],
      "source": [
        "df0 = pd.DataFrame(list(countingDict.items()), columns=['Language', 'Count'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAUk6iuvcMPq"
      },
      "outputs": [],
      "source": [
        "repoByLanguage = {}\n",
        "\n",
        "for key in counting.keys():\n",
        "  repoByLanguage.setdefault(key, [])\n",
        "  for DataTable in jsons:\n",
        "    for elements in jsons[DataTable][\"Sources\"]:\n",
        "      if elements[\"RepoLanguage\"] == key:\n",
        "        repoByLanguage[key].append(elements[\"RepoName\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3FmmUfKuFht"
      },
      "outputs": [],
      "source": [
        "df1 = pd.DataFrame(list(repoByLanguage.items()), columns=['Language', 'Repos'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIwI7ZYt8SWp"
      },
      "outputs": [],
      "source": [
        "repoByLanguageCount = copy.deepcopy(repoByLanguage)\n",
        "\n",
        "for key, alias in repoByLanguage.items():\n",
        "    repoByLanguage[key] = list(set(alias))\n",
        "    repoByLanguageCount[key] = len(list(set(alias)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MqOCA13jk16"
      },
      "outputs": [],
      "source": [
        "df2 = pd.DataFrame(list(repoByLanguage.items()), columns=['Language', 'UniqueRepos'])\n",
        "df3 = pd.DataFrame(list(repoByLanguageCount.items()), columns=['Language', 'UniqueReposCount'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTB8tUaa8ZgH"
      },
      "source": [
        "## CountChat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Gj2Ig-q8bon"
      },
      "outputs": [],
      "source": [
        "chatByLanguage = {}\n",
        "for DataTable in jsons:\n",
        "  for elements in jsons[DataTable][\"Sources\"]:\n",
        "    chatByLanguage.setdefault(elements[\"RepoLanguage\"], [])\n",
        "    for element in elements[\"ChatgptSharing\"]:\n",
        "      chatByLanguage[elements[\"RepoLanguage\"]].append(element[\"URL\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfyDGygyKSc1"
      },
      "outputs": [],
      "source": [
        "df4 = pd.DataFrame(list(chatByLanguage.items()), columns=['Language', 'Chat'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wf782MlrOM11"
      },
      "outputs": [],
      "source": [
        "chatByLanguageCount = copy.deepcopy(chatByLanguage)\n",
        "for key, alias in chatByLanguage.items():\n",
        "    chatByLanguage[key] = list(set(alias))\n",
        "    chatByLanguageCount[key] = len(list(set(alias)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BVGqJuNO5Ar"
      },
      "outputs": [],
      "source": [
        "chatByLanguage\n",
        "df5 = pd.DataFrame(list(chatByLanguage.items()), columns=['Language', 'UniqueChat'])\n",
        "df6 = pd.DataFrame(list(chatByLanguageCount.items()), columns=['Language', 'UniqueChatCount'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA4B5g7kRJQB"
      },
      "source": [
        "## Code Snipets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYF_L6rKRMHM"
      },
      "outputs": [],
      "source": [
        "snipetsByLanguage = {}\n",
        "for DataTable in jsons:\n",
        "  for elements in jsons[DataTable][\"Sources\"]:\n",
        "    snipetsByLanguage.setdefault(elements[\"RepoLanguage\"], [])\n",
        "    for element in elements[\"ChatgptSharing\"]:\n",
        "      if \"Conversations\" in element and element[\"Conversations\"]:\n",
        "        for conversation in element[\"Conversations\"]:\n",
        "          if \"ListOfCode\" in conversation and conversation[\"ListOfCode\"]:\n",
        "            for snipetType in conversation[\"ListOfCode\"]:\n",
        "              snipetsByLanguage[elements[\"RepoLanguage\"]].append(snipetType[\"Type\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdlUVMODhMzz"
      },
      "outputs": [],
      "source": [
        "snipetsByLanguage\n",
        "snipetsByLanguagePercentage = copy.deepcopy(snipetsByLanguage)\n",
        "snipetsByLanguageTotal = copy.deepcopy(snipetsByLanguage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mV5BHs_M1vj2"
      },
      "outputs": [],
      "source": [
        "\n",
        "for key, alias in snipetsByLanguage.items():\n",
        "    total_elements_on_lista = len(alias)\n",
        "\n",
        "    count_elements = Counter(alias)\n",
        "\n",
        "    snipetsByLanguagePercentage[key] = {element: f'{(ocurrency / total_elements_on_lista) * 100:.3f}' for element, ocurrency in count_elements.items()}\n",
        "    snipetsByLanguage[key] = dict(Counter(snipetsByLanguage[key]))\n",
        "    snipetsByLanguageTotal[key] = (sum(Counter(snipetsByLanguage[key]).values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qn1uD23vcto2"
      },
      "outputs": [],
      "source": [
        "df7 = pd.DataFrame(list(snipetsByLanguage.items()), columns=['Language', 'SnippetLang'])\n",
        "\n",
        "df8 = pd.DataFrame(list(snipetsByLanguageTotal.items()), columns=['Language', 'SnippetLangCount'])\n",
        "\n",
        "df9 = pd.DataFrame(list(snipetsByLanguagePercentage.items()), columns=['Language', 'SnippetLangPercentage'])\n",
        "\n",
        "final_df = pd.merge(df0, df1, on='Language').merge(df2, on='Language').merge(df3, on='Language').merge(df4, on='Language')\n",
        "final_df = final_df.merge(df5, on='Language').merge(df6, on='Language').merge(df7, on='Language').merge(df8, on='Language').merge(df9, on='Language')\n",
        "final_df.to_csv('results/merged_results_table.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k59i_LgyEzBg"
      },
      "source": [
        "## Verify Snipets by language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQDjZn-VsvqY"
      },
      "outputs": [],
      "source": [
        "\n",
        "rows = []\n",
        "for columns, values in snipetsByLanguage.items():\n",
        "    row = {'language': columns}\n",
        "    row.update(values)\n",
        "    rows.append(row)\n",
        "\n",
        "snipetsByLanguageDF = pd.DataFrame(rows)\n",
        "snipetsByLanguageDF = snipetsByLanguageDF.T\n",
        "\n",
        "\n",
        "snipetsByLanguageDF = snipetsByLanguageDF.fillna(0)\n",
        "snipetsByLanguageDF.to_csv('results/merged_snipetsByLanguageDF.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Snipets By Language in Percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH1qo3Mq1I-f"
      },
      "outputs": [],
      "source": [
        "rows = []\n",
        "for columns, values in snipetsByLanguagePercentage.items():\n",
        "    row = {'language': columns}\n",
        "    row.update(values)\n",
        "    rows.append(row)\n",
        "\n",
        "\n",
        "snipetsByLanguagePercentageDF = pd.DataFrame(rows)\n",
        "snipetsByLanguagePercentageDF = snipetsByLanguagePercentageDF.T\n",
        "\n",
        "\n",
        "snipetsByLanguagePercentageDF = snipetsByLanguagePercentageDF.fillna(0)\n",
        "snipetsByLanguagePercentageDF.to_csv('results/merged_snipetsByLanguagePercentageDF.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
